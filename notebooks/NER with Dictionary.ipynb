{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER with Dictionary of Diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Add functions in lib folder\n",
    "import sys\n",
    "import os\n",
    "module_path = os.path.abspath(os.path.join('../code'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from lib.DataProcess import DataProcess\n",
    "from lib.Jaccard import Jaccard\n",
    "\n",
    "import spacy\n",
    "from spacy.symbols import ORTH, LEMMA, POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 693\n",
      "Diseases dictionary: 316\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = '../data/'\n",
    "\n",
    "train_file = 'NCBI_corpus_training.txt'\n",
    "test_file = 'NCBI_corpus_testing.txt'\n",
    "diseases_file = 'diseases.txt'\n",
    "\n",
    "with open(DATA_PATH + train_file, 'r') as fp:\n",
    "    train_dataset = fp.readlines()\n",
    "\n",
    "with open(DATA_PATH + test_file, 'r') as fp:\n",
    "    test_dataset = fp.readlines()\n",
    "\n",
    "dataset = train_dataset + test_dataset\n",
    "\n",
    "with open(DATA_PATH + diseases_file, 'r') as fp:\n",
    "    diseases = fp.readlines()\n",
    "\n",
    "print('Dataset: %d' % len(dataset))\n",
    "print('Diseases dictionary: %d' % len(diseases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard = Jaccard()\n",
    "data_process = DataProcess(jaccard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "This is a minimal preprocess of the data, as I do not want to remove any essential information from texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that I am using contains a number in the beginning of every text, thus I need to remove that number. Also, I replace the `category` tags with `<entity>`, so I can add them to the vocabular of Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identification of APC2, a homologue of the <entity>adenomatous polyposis coli tumour</entity> suppressor .\tThe <entity>adenomatous polyposis coli ( APC ) tumour</entity>-suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta ( GSK-3beta ) , axin / conductin and betacatenin . Complex formation induces the rapid degradation of betacatenin . In <entity>colon carcinoma</entity> cells , loss of APC leads to the accumulation of betacatenin in the nucleus , where it binds to and activates the Tcf-4 transcription factor ( reviewed in [ 1 ] [ 2 ] ) . Here , we report the identification and genomic structure of APC homologues . Mammalian APC2 , which closely resembles APC in overall domain structure , was functionally analyzed and shown to contain two SAMP domains , both of which are required for binding to conductin . Like APC , APC2 regulates the formation of active betacatenin-Tcf complexes , as demonstrated using transient transcriptional activation assays in APC - / - <entity>colon carcinoma</entity> cells . Human APC2 maps to chromosome 19p13 . 3 . APC and APC2 may therefore have comparable functions in development and <entity>cancer</entity> .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = data_process.apply_initial_cleaner(dataset)\n",
    "\n",
    "print(dataset[0]) # Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I split the texts using the **Spacy Tokenizer**. Note that I split not only the words but also the sentences, so I consider each sentence as an independent input of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('identification', 'NOUN', Identification), ('of', 'ADP', of), ('apc2', 'PROPN', APC2), (',', 'PUNCT', ,), ('a', 'DET', a), ('homologue', 'NOUN', homologue), ('of', 'ADP', of), ('the', 'DET', the), ('<entity>', 'X', <entity>), ('adenomatous', 'ADJ', adenomatous), ('polyposis', 'NOUN', polyposis), ('coli', 'NOUN', coli), ('tumour', 'NOUN', tumour), ('</entity>', 'X', </entity>), ('suppressor', 'NOUN', suppressor), ('.', 'PUNCT', .)]\n"
     ]
    }
   ],
   "source": [
    "tok_dataset = data_process.tokenize_texts(dataset)\n",
    "\n",
    "print(tok_dataset[0]) # Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I transform the sequence of words into 0s and 1s such that 1s mean that it is an **entity** and 0s mean that it is just a **common word**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 1 1 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "indicators = data_process.get_indicator_sequences(tok_dataset)\n",
    "\n",
    "print(indicators[0]) # Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is necessary to tokenize the dictionary of diseases too\n",
    "tok_diseases = data_process.tokenize_texts(diseases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_entity_tags(tok_dataset):\n",
    "    return [[token for token in text if token[2].text not in [DataProcess.ENTITY_START, DataProcess.ENTITY_END]] for text in tok_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_dataset = remove_entity_tags(tok_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use dictionary\n",
    "\n",
    "We are going to use the dictionary to find diseases. Note that I find the entities by using the Jaccard Index, so first I should adjust the `min_score` value. This values behaves as threshold to mark a string as entity or not.\n",
    "\n",
    "I evaluate the model by computing the Jaccard Index with the pseudo-binary sequences. Details: https://en.wikipedia.org/wiki/Jaccard_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entities(tok_dataset, tok_entities, min_score=0.5, debug=False):\n",
    "    entities_per_text = []\n",
    "    indicators_per_text = []\n",
    "    \n",
    "    for log_i, text in enumerate(tok_dataset):\n",
    "        text_len = len(text)\n",
    "        t_entities = []\n",
    "        t_indicator = []\n",
    "        \n",
    "        if log_i % 100 == 0:\n",
    "            print('- Text %d of %d' % (log_i, len(tok_dataset)))\n",
    "        \n",
    "        i = 0\n",
    "        while i < text_len:\n",
    "            entity_found = False\n",
    "            \n",
    "            for entity in tok_entities:\n",
    "                entity_len = len(entity)\n",
    "                score = 0\n",
    "                \n",
    "                if entity_len + i > text_len:\n",
    "                    # The entity cannot fit in the tokenized words\n",
    "                    continue\n",
    "\n",
    "                k = 0\n",
    "                while k < entity_len:\n",
    "                    score = score + jaccard.word_jaccard(entity[k][2].text, text[i + k][2].text)\n",
    "                    k = k + 1\n",
    "                \n",
    "                score = score / entity_len\n",
    "                if score >= min_score:\n",
    "                    entity_found = True\n",
    "                    t_entities.append(text[i:i+k])\n",
    "                    t_indicator = t_indicator + [1]*k\n",
    "                    i = i + k\n",
    "            \n",
    "            if not entity_found:\n",
    "                t_indicator.append(0)\n",
    "                i = i + 1\n",
    "        \n",
    "        entities_per_text.append(np.array(t_entities))\n",
    "        indicators_per_text.append(np.array(t_indicator))\n",
    "    \n",
    "    return np.array(entities_per_text), np.array(indicators_per_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Try to find the optimal threshold value\n",
    "# min_score_values = np.arange(0.5, 0.8, 0.1)\n",
    "min_score_values = [0.7]\n",
    "\n",
    "best_min_score = 0\n",
    "best_jaccard_score = 0\n",
    "\n",
    "for min_score in min_score_values:\n",
    "    print('Threshold: %f' % min_score)\n",
    "    \n",
    "    _, pred_indicators = find_entities(tok_dataset, tok_diseases, min_score, debug=True)\n",
    "    jaccard_score = jaccard.bin_jaccard(np.array(indicators), pred_indicators)\n",
    "    \n",
    "    print('- Jaccard: %.4f' % jaccard_score)\n",
    "    \n",
    "    if jaccard_score > best_jaccard_score:\n",
    "        best_min_score = min_score\n",
    "        best_jaccard_score = jaccard_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Jaccard score: %.4f' % best_jaccard_score)\n",
    "print('Threshold: %d' % best_min_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
