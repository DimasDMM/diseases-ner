{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER with LSTM-CRF\n",
    "\n",
    "It is necessary to install not only tensorflow and keras but also keras-contrib to use this notebook: `pip install git+https://www.github.com/keras-team/keras-contrib.git`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import en_core_web_sm\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed, Reshape, Bidirectional, concatenate, Flatten\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.metrics import crf_accuracy\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'\n",
    "ARTIFACTS_PATH = '../artifacts/'\n",
    "\n",
    "PARSED_DATA_PATH = DATA_PATH + 'parsed/'\n",
    "SPLIT_CHAR = ' '\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "\n",
    "MODEL_NAME = 'ner-lstm-crf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "The files `bin_corpus_<>` contain the same texts but as a numbers, where $1$ means that it is a medical term and $0$ that it is just a \"common\" word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_file = 'corpus_train.txt'\n",
    "corpus_test_file = 'corpus_test.txt'\n",
    "bin_corpus_train_file = 'bin_corpus_train.txt'\n",
    "bin_corpus_test_file = 'bin_corpus_test.txt'\n",
    "\n",
    "with open(PARSED_DATA_PATH + corpus_train_file, 'r') as fp:\n",
    "    train_dataset = fp.readlines()\n",
    "\n",
    "with open(PARSED_DATA_PATH + corpus_test_file, 'r') as fp:\n",
    "    test_dataset = fp.readlines()\n",
    "\n",
    "with open(PARSED_DATA_PATH + bin_corpus_train_file, 'r') as fp:\n",
    "    bin_train_dataset = fp.readlines()\n",
    "    bin_train_dataset = [s.strip().split(SPLIT_CHAR) if s.strip() != '' else [] for s in bin_train_dataset]\n",
    "    bin_train_dataset = [np.array(s).astype('int') for s in bin_train_dataset]\n",
    "\n",
    "with open(PARSED_DATA_PATH + bin_corpus_test_file, 'r') as fp:\n",
    "    bin_test_dataset = fp.readlines()\n",
    "    bin_test_dataset = [s.strip().split(SPLIT_CHAR) if s.strip() != '' else [] for s in bin_test_dataset]\n",
    "    bin_test_dataset = [np.array(s).astype('int') for s in bin_test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nlp_engine(basic_tokenizer=False):\n",
    "    if basic_tokenizer:\n",
    "        nlp = English()\n",
    "    else:\n",
    "        nlp = en_core_web_sm.load()\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(texts, basic_tokenizer=False):\n",
    "    texts = texts.copy()\n",
    "    nlp = get_nlp_engine(basic_tokenizer=basic_tokenizer)\n",
    "    \n",
    "    for index, text in enumerate(texts):\n",
    "        nlp_tokens = nlp(text)\n",
    "        \n",
    "        pos = [w.pos_ for w in nlp_tokens]\n",
    "        lemmas = [w.lemma_.strip() if w.lemma_ != \"-PRON-\" else w.lower_.strip() for w in nlp_tokens]\n",
    "        lemmas = [w if any(c.isupper() for c in w) else w for w in lemmas]\n",
    "        \n",
    "        # Remove empty tokens\n",
    "        tokens = [(lemmas[i].strip(), pos[i], nlp_tokens[i]) for i in range(len(nlp_tokens)) if lemmas[i].strip() != '']\n",
    "        \n",
    "        texts[index] = tokens\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenize_texts(train_dataset)\n",
    "test_dataset = tokenize_texts(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_train_dataset = [[w[0] for w in text] for text in train_dataset]\n",
    "tok_test_dataset = [[w[0] for w in text] for text in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train_dataset = [[w[1] for w in text] for text in train_dataset]\n",
    "pos_test_dataset = [[w[1] for w in text] for text in test_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to encode the list of strings into a list of numbers such as every number corresponds unequivocally to a string. Keras has already implemented a function for this purpose: https://keras.io/api/preprocessing/text/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_tokenizer(data, num_words=150000):\n",
    "    tokenizer = Tokenizer(num_words=num_words)\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_sequences(data, max_sequence_length, tokenizer=None):\n",
    "    if tokenizer:\n",
    "        data = tokenizer.texts_to_sequences(data)\n",
    "\n",
    "    data = sequence.pad_sequences(data, maxlen=max_sequence_length, padding='post')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_dataset = tok_train_dataset + tok_test_dataset\n",
    "word_tokenizer = get_keras_tokenizer(tok_dataset)\n",
    "\n",
    "pos_dataset = pos_train_dataset + pos_test_dataset\n",
    "pos_tokenizer = get_keras_tokenizer(pos_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_train_dataset = data_to_sequences(tok_train_dataset, MAX_SEQUENCE_LENGTH, word_tokenizer)\n",
    "tok_test_dataset = data_to_sequences(tok_test_dataset, MAX_SEQUENCE_LENGTH, word_tokenizer)\n",
    "\n",
    "pos_train_dataset = data_to_sequences(pos_train_dataset, MAX_SEQUENCE_LENGTH, pos_tokenizer)\n",
    "pos_test_dataset = data_to_sequences(pos_test_dataset, MAX_SEQUENCE_LENGTH, pos_tokenizer)\n",
    "\n",
    "bin_train_dataset = data_to_sequences(bin_train_dataset, MAX_SEQUENCE_LENGTH)\n",
    "bin_test_dataset = data_to_sequences(bin_test_dataset, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training the network we also need to change the bin_<>_dataset to categorial.\n",
    "bincat_train_dataset = np.array([to_categorical(i, num_classes=2) for i in bin_train_dataset])\n",
    "bincat_test_dataset = np.array([to_categorical(i, num_classes=2) for i in bin_test_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model\n",
    "\n",
    "The following model is based on the paper of *Zhiheng Huang et al.*: https://arxiv.org/pdf/1508.01991.pdf\n",
    "\n",
    "In this case I have two different outputs:\n",
    "- The string sequence.\n",
    "- The PoS tags sequence.\n",
    "\n",
    "Remember that both sequences must be numericals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model_params: dictionary:\n",
    "- embedding_dim\n",
    "- lstm_cells\n",
    "- word_lstm_dropout\n",
    "- word_lstm_rec_dropout\n",
    "- pos_lstm_dropout\n",
    "- pos_lstm_rec_dropout\n",
    "\"\"\"\n",
    "def create_model(model_params, word_tokenizer, pos_tokenizer, sequence_length):\n",
    "    word_index = word_tokenizer.word_index\n",
    "    pos_index = pos_tokenizer.word_index\n",
    "\n",
    "    word_input = Input(shape=(sequence_length,), name='word_input')\n",
    "    word_pipe = Embedding(input_dim=len(word_index) + 1,\n",
    "                          output_dim=model_params['embedding_dim'],\n",
    "                          input_length=sequence_length,\n",
    "                          trainable=True)(word_input)\n",
    "    word_pipe = Bidirectional(\n",
    "                    LSTM(model_params['lstm_cells'],\n",
    "                         return_sequences=True,\n",
    "                         dropout=model_params['word_lstm_dropout'],\n",
    "                         recurrent_dropout=model_params['word_lstm_rec_dropout']),\n",
    "                    merge_mode='concat')(word_pipe)\n",
    "    word_pipe = TimeDistributed(Flatten())(word_pipe)\n",
    "\n",
    "    pos_input = Input(shape=(sequence_length,), name='pos_input')\n",
    "    pos_pipe = Embedding(input_dim=len(pos_index) + 1,\n",
    "                         output_dim=model_params['embedding_dim'],\n",
    "                         input_length=sequence_length,\n",
    "                         trainable=True)(pos_input)\n",
    "    pos_pipe = Bidirectional(\n",
    "                    LSTM(model_params['lstm_cells'],\n",
    "                         return_sequences=True,\n",
    "                         dropout=model_params['pos_lstm_dropout'],\n",
    "                         recurrent_dropout=model_params['pos_lstm_rec_dropout']),\n",
    "                    merge_mode='concat')(pos_pipe)\n",
    "    pos_pipe = TimeDistributed(Flatten())(pos_pipe)\n",
    "    \n",
    "    # Concatenate both inputs\n",
    "    comb_pipe = concatenate([word_pipe, pos_pipe])\n",
    "\n",
    "    # Main BiLSTM model\n",
    "    comb_pipe = Bidirectional(\n",
    "        LSTM(model_params['lstm_cells'], return_sequences=True),\n",
    "        merge_mode='concat')(comb_pipe)\n",
    "    comb_pipe = TimeDistributed(Dense(64))(comb_pipe)\n",
    "    \n",
    "    output = CRF(2, name='output')(comb_pipe)\n",
    "    \n",
    "    model = Model(inputs=[word_input, pos_input], outputs=output)\n",
    "    model.compile(\n",
    "        loss=crf_loss,\n",
    "        optimizer='adam',\n",
    "        metrics=[crf_accuracy]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "I did not optimize the following parameters. If you want to get the best possible results, use some library to find the optimal parameter values: https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview#whats_a_hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "word_input (InputLayer)         (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pos_input (InputLayer)          (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 50, 100)      875600      word_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 50, 100)      1800        pos_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 50, 100)      60400       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 50, 100)      60400       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 50, 100)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 50, 100)      0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 50, 200)      0           time_distributed_1[0][0]         \n",
      "                                                                 time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 50, 100)      100400      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 50, 64)       6464        bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "output (CRF)                    (None, 50, 2)        138         time_distributed_3[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 1,105,202\n",
      "Trainable params: 1,105,202\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_params = {\n",
    "    'embedding_dim': 100,\n",
    "    'lstm_cells': 50,\n",
    "    'word_lstm_dropout': 0.3,\n",
    "    'word_lstm_rec_dropout': 0.3,\n",
    "    'pos_lstm_dropout': 0.3,\n",
    "    'pos_lstm_rec_dropout': 0.3\n",
    "}\n",
    "\n",
    "model = create_model(model_params, word_tokenizer, pos_tokenizer, MAX_SEQUENCE_LENGTH)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dimasdmm/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:430: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7771/7771 [==============================] - 60s 8ms/step - loss: 0.0749 - crf_accuracy: 0.9837\n",
      "Epoch 2/5\n",
      "7771/7771 [==============================] - 52s 7ms/step - loss: 0.0368 - crf_accuracy: 0.9928\n",
      "Epoch 3/5\n",
      "7771/7771 [==============================] - 43s 5ms/step - loss: 0.0277 - crf_accuracy: 0.9949\n",
      "Epoch 4/5\n",
      "7771/7771 [==============================] - 44s 6ms/step - loss: 0.0205 - crf_accuracy: 0.9963\n",
      "Epoch 5/5\n",
      "7771/7771 [==============================] - 42s 5ms/step - loss: 0.0141 - crf_accuracy: 0.9972\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "# Add early stop\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)\n",
    "\n",
    "history = model.fit(\n",
    "    {'word_input': tok_train_dataset, 'pos_input': pos_train_dataset},\n",
    "    bincat_train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalutaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bincat_test_dataset = model.predict({'word_input': tok_test_dataset, 'pos_input': pos_test_dataset})\n",
    "pred_bin_test_dataset = [np.argmax(s, axis=1) for s in pred_bincat_test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(real_bin_dataset, pred_bin_dataset):\n",
    "    if len(real_bin_dataset) != len(pred_bin_dataset):\n",
    "        raise Exception('Different lengths in lists: %d vs %d' % (len(real_bin_dataset), len(pred_bin_dataset)))\n",
    "    \n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tp = 0\n",
    "    \n",
    "    text_i = 0\n",
    "    len_texts = len(real_bin_dataset)\n",
    "    \n",
    "    while text_i < len_texts:\n",
    "        token_i = 0\n",
    "        len_text = len(real_bin_dataset[text_i])\n",
    "        \n",
    "        while token_i < len_text:\n",
    "            if real_bin_dataset[text_i][token_i] == 1:\n",
    "                tp = tp + int(pred_bin_dataset[text_i][token_i] == 1)\n",
    "                fn = fn + int(pred_bin_dataset[text_i][token_i] == 0)\n",
    "            else:\n",
    "                fp = fp + int(pred_bin_dataset[text_i][token_i] == 1)\n",
    "                tn = tn + int(pred_bin_dataset[text_i][token_i] == 0)\n",
    "            token_i = token_i + 1\n",
    "\n",
    "        text_i = text_i + 1\n",
    "    \n",
    "    return np.array([[tn, fp], [fn, tp]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(conf_matrix, normalize=True):\n",
    "    conf_matrix = copy.deepcopy(conf_matrix)\n",
    "    \n",
    "    if normalize:\n",
    "        conf_matrix = np.around(conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "    \n",
    "    label_names = ['Bad', 'Good']\n",
    "    df_confusion_matrix = pd.DataFrame(conf_matrix,\n",
    "                                       index=label_names, \n",
    "                                       columns=label_names)\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sns.heatmap(df_confusion_matrix, cmap=plt.cm.Blues, annot=True)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(bin_test_dataset, pred_bin_test_dataset)\n",
    "plot_confusion_matrix(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From my point of view, the results are nice: I barely get any false positive, thus I do not identify \"false entities\" in the text. However, I should keep working on my model to detect all the entities, since there are $20\\%$ of them that I am not identifying correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction\n",
    "\n",
    "Here I do some predictions and transform the categorical output into the entity strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bin_pred(model, nlp_text):\n",
    "    tok_texts = list([[w[0] for w in nlp_text]])\n",
    "    pos_texts = list([[w[1] for w in nlp_text]])\n",
    "\n",
    "    tok_texts = data_to_sequences(tok_texts, MAX_SEQUENCE_LENGTH, word_tokenizer)\n",
    "    pos_texts = data_to_sequences(pos_texts, MAX_SEQUENCE_LENGTH, pos_tokenizer)\n",
    "\n",
    "    # Make the prediction\n",
    "    pred_entities = model.predict({'word_input': tok_texts, 'pos_input': pos_texts})\n",
    "    pred_entities = [np.argmax(s, axis=1) for s in pred_entities]\n",
    "    \n",
    "    return pred_entities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_pred_to_str(nlp_text, bin_pred):\n",
    "    i = 0\n",
    "    \n",
    "    text_length = len(nlp_text)\n",
    "    entities = []\n",
    "\n",
    "    # Get list of entities separately\n",
    "    entities_stack = []\n",
    "    while i < text_length:\n",
    "        if bin_pred[i]:\n",
    "            entities_stack.append((i, nlp_text[i][2].text_with_ws))\n",
    "        i = i + 1\n",
    "\n",
    "    # Concatenate consecutive entities\n",
    "    last_pos = None\n",
    "    for e in entities_stack:\n",
    "        if last_pos == e[0] - 1:\n",
    "            last_pos = e[0]\n",
    "            n_entities = len(entities) - 1\n",
    "            entities[n_entities] = entities[n_entities] + e[1]\n",
    "        else:\n",
    "            last_pos = e[0]\n",
    "            entities.append(e[1])\n",
    "\n",
    "    entities = [e.strip() for e in entities]        \n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(text, model):\n",
    "    # Put the previous text inside of a list\n",
    "    texts = list([text])\n",
    "\n",
    "    # Tokenize it and get PoS tags\n",
    "    nlp_text = tokenize_texts(texts)[0]\n",
    "\n",
    "    pred_bin_text = make_bin_pred(model, nlp_text)\n",
    "    pred_str_entities = bin_pred_to_str(nlp_text, pred_bin_text)\n",
    "    \n",
    "    return pred_str_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['breast and ovarian cancer', 'tumor']\n"
     ]
    }
   ],
   "source": [
    "text = 'Germ-line mutations of the BRCA1 gene predispose women to early-onset breast '\\\n",
    "       'and ovarian cancer by compromising the genes presumptive function as a tumor suppressor'\n",
    "\n",
    "pred_str_entities = get_entities(text, model)\n",
    "\n",
    "print(pred_str_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scoliosis']\n"
     ]
    }
   ],
   "source": [
    "text = 'The patient has been diagnosed with scoliosis.'\n",
    "\n",
    "pred_str_entities = get_entities(text, model)\n",
    "\n",
    "print(pred_str_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_dir(model_name):\n",
    "    model_path = ARTIFACTS_PATH + model_name + '/'\n",
    "    if not os.path.exists(model_path):\n",
    "        os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_name, model):\n",
    "    create_model_dir(model_name)\n",
    "    model_path = ARTIFACTS_PATH + model_name + '/'\n",
    "    model.save_weights(model_path + 'model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokenizer(model_name, word_tokenizer, pos_tokenizer):\n",
    "    create_model_dir(model_name)\n",
    "    model_path = ARTIFACTS_PATH + model_name + '/'\n",
    "    \n",
    "    with open(model_path + 'word_tokenizer.pickle', 'wb') as fp:\n",
    "        pickle.dump(word_tokenizer, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    with open(model_path + 'pos_tokenizer.pickle', 'wb') as fp:\n",
    "        pickle.dump(pos_tokenizer, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "save_model(MODEL_NAME, model)\n",
    "save_tokenizer(MODEL_NAME, word_tokenizer, pos_tokenizer)\n",
    "print('Saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
