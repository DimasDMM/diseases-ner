{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER with LSTM-CRF\n",
    "\n",
    "It is necessary to install not only tensorflow and keras but also keras-contrib to use this notebook: `pip install git+https://www.github.com/keras-team/keras-contrib.git`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import en_core_web_sm\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed, Reshape, Bidirectional, concatenate, Flatten\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.metrics import crf_accuracy\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'\n",
    "ARTIFACTS_PATH = '../artifacts/'\n",
    "\n",
    "PARSED_DATA_PATH = DATA_PATH + 'parsed/'\n",
    "SPLIT_CHAR = ' '\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "\n",
    "MODEL_NAME = 'ner-lstm-crf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "The files `bin_corpus_<>` contain the same texts but as a numbers, where $1$ means that it is a medical term and $0$ that it is just a \"common\" word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_file = 'corpus_train.txt'\n",
    "corpus_test_file = 'corpus_test.txt'\n",
    "bin_corpus_train_file = 'bin_corpus_train.txt'\n",
    "bin_corpus_test_file = 'bin_corpus_test.txt'\n",
    "\n",
    "with open(PARSED_DATA_PATH + corpus_train_file, 'r') as fp:\n",
    "    train_dataset = fp.readlines()\n",
    "\n",
    "with open(PARSED_DATA_PATH + corpus_test_file, 'r') as fp:\n",
    "    test_dataset = fp.readlines()\n",
    "\n",
    "with open(PARSED_DATA_PATH + bin_corpus_train_file, 'r') as fp:\n",
    "    bin_train_dataset = fp.readlines()\n",
    "    bin_train_dataset = [s.strip().split(SPLIT_CHAR) if s.strip() != '' else [] for s in bin_train_dataset]\n",
    "    bin_train_dataset = [np.array(s).astype('int') for s in bin_train_dataset]\n",
    "\n",
    "with open(PARSED_DATA_PATH + bin_corpus_test_file, 'r') as fp:\n",
    "    bin_test_dataset = fp.readlines()\n",
    "    bin_test_dataset = [s.strip().split(SPLIT_CHAR) if s.strip() != '' else [] for s in bin_test_dataset]\n",
    "    bin_test_dataset = [np.array(s).astype('int') for s in bin_test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nlp_engine(basic_tokenizer=False):\n",
    "    if basic_tokenizer:\n",
    "        nlp = English()\n",
    "    else:\n",
    "        nlp = en_core_web_sm.load()\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(texts, basic_tokenizer=False):\n",
    "    texts = texts.copy()\n",
    "    nlp = get_nlp_engine(basic_tokenizer=basic_tokenizer)\n",
    "    \n",
    "    for index, text in enumerate(texts):\n",
    "        nlp_tokens = nlp(text)\n",
    "        \n",
    "        pos = [w.pos_ for w in nlp_tokens]\n",
    "        lemmas = [w.lemma_.strip() if w.lemma_ != \"-PRON-\" else w.lower_.strip() for w in nlp_tokens]\n",
    "        lemmas = [w if any(c.isupper() for c in w) else w for w in lemmas]\n",
    "        \n",
    "        # Remove empty tokens\n",
    "        tokens = [(lemmas[i].strip(), pos[i], nlp_tokens[i]) for i in range(len(nlp_tokens)) if lemmas[i].strip() != '']\n",
    "        \n",
    "        texts[index] = tokens\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenize_texts(train_dataset)\n",
    "test_dataset = tokenize_texts(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_train_dataset = [[w[0] for w in text] for text in train_dataset]\n",
    "tok_test_dataset = [[w[0] for w in text] for text in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train_dataset = [[w[1] for w in text] for text in train_dataset]\n",
    "pos_test_dataset = [[w[1] for w in text] for text in test_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to encode the list of strings into a list of numbers such as every number corresponds unequivocally to a string. Keras has already implemented a function for this purpose: https://keras.io/api/preprocessing/text/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_tokenizer(data, num_words=150000):\n",
    "    tokenizer = Tokenizer(num_words=num_words)\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_sequences(data, max_sequence_length, tokenizer=None):\n",
    "    if tokenizer:\n",
    "        data = tokenizer.texts_to_sequences(data)\n",
    "\n",
    "    data = sequence.pad_sequences(data, maxlen=max_sequence_length, padding='post')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_dataset = tok_train_dataset + tok_test_dataset\n",
    "word_tokenizer = get_keras_tokenizer(tok_dataset)\n",
    "\n",
    "pos_dataset = pos_train_dataset + pos_test_dataset\n",
    "pos_tokenizer = get_keras_tokenizer(pos_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_train_dataset = data_to_sequences(tok_train_dataset, MAX_SEQUENCE_LENGTH, word_tokenizer)\n",
    "tok_test_dataset = data_to_sequences(tok_test_dataset, MAX_SEQUENCE_LENGTH, word_tokenizer)\n",
    "\n",
    "pos_train_dataset = data_to_sequences(pos_train_dataset, MAX_SEQUENCE_LENGTH, pos_tokenizer)\n",
    "pos_test_dataset = data_to_sequences(pos_test_dataset, MAX_SEQUENCE_LENGTH, pos_tokenizer)\n",
    "\n",
    "bin_train_dataset = data_to_sequences(bin_train_dataset, MAX_SEQUENCE_LENGTH)\n",
    "bin_test_dataset = data_to_sequences(bin_test_dataset, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training the network we also need to change the bin_<>_dataset to categorial.\n",
    "bincat_train_dataset = np.array([to_categorical(i, num_classes=2) for i in bin_train_dataset])\n",
    "bincat_test_dataset = np.array([to_categorical(i, num_classes=2) for i in bin_test_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model\n",
    "\n",
    "The following model is based on the paper of *Zhiheng Huang et al.*: https://arxiv.org/pdf/1508.01991.pdf\n",
    "\n",
    "In this case I have two different outputs:\n",
    "- The string sequence.\n",
    "- The PoS tags sequence.\n",
    "\n",
    "Remember that both sequences must be numericals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model_params: dictionary:\n",
    "- embedding_dim\n",
    "- lstm_cells\n",
    "- word_lstm_dropout\n",
    "- word_lstm_rec_dropout\n",
    "- pos_lstm_dropout\n",
    "- pos_lstm_rec_dropout\n",
    "\"\"\"\n",
    "def create_model(model_params, word_tokenizer, pos_tokenizer, sequence_length):\n",
    "    word_index = word_tokenizer.word_index\n",
    "    pos_index = pos_tokenizer.word_index\n",
    "\n",
    "    word_input = Input(shape=(sequence_length,), name='word_input')\n",
    "    word_pipe = Embedding(input_dim=len(word_index) + 1,\n",
    "                          output_dim=model_params['embedding_dim'],\n",
    "                          input_length=sequence_length,\n",
    "                          trainable=True)(word_input)\n",
    "    word_pipe = Bidirectional(\n",
    "                    LSTM(model_params['lstm_cells'],\n",
    "                         return_sequences=True,\n",
    "                         dropout=model_params['word_lstm_dropout'],\n",
    "                         recurrent_dropout=model_params['word_lstm_rec_dropout']),\n",
    "                    merge_mode='concat')(word_pipe)\n",
    "    word_pipe = TimeDistributed(Flatten())(word_pipe)\n",
    "\n",
    "    pos_input = Input(shape=(sequence_length,), name='pos_input')\n",
    "    pos_pipe = Embedding(input_dim=len(pos_index) + 1,\n",
    "                         output_dim=model_params['embedding_dim'],\n",
    "                         input_length=sequence_length,\n",
    "                         trainable=True)(pos_input)\n",
    "    pos_pipe = Bidirectional(\n",
    "                    LSTM(model_params['lstm_cells'],\n",
    "                         return_sequences=True,\n",
    "                         dropout=model_params['pos_lstm_dropout'],\n",
    "                         recurrent_dropout=model_params['pos_lstm_rec_dropout']),\n",
    "                    merge_mode='concat')(pos_pipe)\n",
    "    pos_pipe = TimeDistributed(Flatten())(pos_pipe)\n",
    "    \n",
    "    # Concatenate both inputs\n",
    "    comb_pipe = concatenate([word_pipe, pos_pipe])\n",
    "\n",
    "    # Main BiLSTM model\n",
    "    comb_pipe = Bidirectional(\n",
    "        LSTM(model_params['lstm_cells'], return_sequences=True),\n",
    "        merge_mode='concat')(comb_pipe)\n",
    "    comb_pipe = TimeDistributed(Dense(64))(comb_pipe)\n",
    "    \n",
    "    output = CRF(2, name='output')(comb_pipe)\n",
    "    \n",
    "    model = Model(inputs=[word_input, pos_input], outputs=output)\n",
    "    model.compile(\n",
    "        loss=crf_loss,\n",
    "        optimizer='adam',\n",
    "        metrics=[crf_accuracy]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "I did not optimize the following parameters. If you want to get the best possible results, use some library to find the optimal parameter values: https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview#whats_a_hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "word_input (InputLayer)         (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pos_input (InputLayer)          (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 50, 100)      875600      word_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 50, 100)      1800        pos_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 50, 100)      60400       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 50, 100)      60400       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 50, 100)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 50, 100)      0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 50, 200)      0           time_distributed_1[0][0]         \n",
      "                                                                 time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 50, 100)      100400      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 50, 64)       6464        bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "output (CRF)                    (None, 50, 2)        138         time_distributed_3[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 1,105,202\n",
      "Trainable params: 1,105,202\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_params = {\n",
    "    'embedding_dim': 100,\n",
    "    'lstm_cells': 50,\n",
    "    'word_lstm_dropout': 0.3,\n",
    "    'word_lstm_rec_dropout': 0.3,\n",
    "    'pos_lstm_dropout': 0.3,\n",
    "    'pos_lstm_rec_dropout': 0.3\n",
    "}\n",
    "\n",
    "model = create_model(model_params, word_tokenizer, pos_tokenizer, MAX_SEQUENCE_LENGTH)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dimas\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7771/7771 [==============================] - 49s 6ms/step - loss: 0.0531 - crf_accuracy: 0.9839\n",
      "Epoch 2/5\n",
      "7771/7771 [==============================] - 32s 4ms/step - loss: 0.0227 - crf_accuracy: 0.9933\n",
      "Epoch 3/5\n",
      "7771/7771 [==============================] - 32s 4ms/step - loss: 0.0133 - crf_accuracy: 0.9956\n",
      "Epoch 4/5\n",
      "7771/7771 [==============================] - 32s 4ms/step - loss: 0.0055 - crf_accuracy: 0.9969\n",
      "Epoch 5/5\n",
      "7771/7771 [==============================] - 32s 4ms/step - loss: -0.0010 - crf_accuracy: 0.9977\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "# Add early stop\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)\n",
    "\n",
    "history = model.fit(\n",
    "    {'word_input': tok_train_dataset, 'pos_input': pos_train_dataset},\n",
    "    bincat_train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalutaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bincat_test_dataset = model.predict({'word_input': tok_test_dataset, 'pos_input': pos_test_dataset})\n",
    "pred_bin_test_dataset = [np.argmax(s, axis=1) for s in pred_bincat_test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(real_bin_dataset, pred_bin_dataset):\n",
    "    if len(real_bin_dataset) != len(pred_bin_dataset):\n",
    "        raise Exception('Different lengths in lists: %d vs %d' % (len(real_bin_dataset), len(pred_bin_dataset)))\n",
    "    \n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tp = 0\n",
    "    \n",
    "    text_i = 0\n",
    "    len_texts = len(real_bin_dataset)\n",
    "    \n",
    "    while text_i < len_texts:\n",
    "        token_i = 0\n",
    "        len_text = len(real_bin_dataset[text_i])\n",
    "        \n",
    "        while token_i < len_text:\n",
    "            if real_bin_dataset[text_i][token_i] == 1:\n",
    "                tp = tp + int(pred_bin_dataset[text_i][token_i] == 1)\n",
    "                fn = fn + int(pred_bin_dataset[text_i][token_i] == 0)\n",
    "            else:\n",
    "                fp = fp + int(pred_bin_dataset[text_i][token_i] == 1)\n",
    "                tn = tn + int(pred_bin_dataset[text_i][token_i] == 0)\n",
    "            token_i = token_i + 1\n",
    "\n",
    "        text_i = text_i + 1\n",
    "    \n",
    "    return np.array([[tn, fp], [fn, tp]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(conf_matrix, normalize=True):\n",
    "    conf_matrix = copy.deepcopy(conf_matrix)\n",
    "    \n",
    "    if normalize:\n",
    "        conf_matrix = np.around(conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "    \n",
    "    label_names = ['Bad', 'Good']\n",
    "    df_confusion_matrix = pd.DataFrame(conf_matrix,\n",
    "                                       index=label_names, \n",
    "                                       columns=label_names)\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sns.heatmap(df_confusion_matrix, cmap=plt.cm.Blues, annot=True)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(bin_test_dataset, pred_bin_test_dataset)\n",
    "plot_confusion_matrix(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From my point of view, the results are nice: I do not get any false positive, thus I do not identify \"false entities\" in the text. However, I should keep working on my model to detect all the entities, since there are $34\\%$ of them that I am not identifying correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction\n",
    "\n",
    "Here I do some predictions and transform the categorical output into the entity strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bin_pred(model, nlp_text):\n",
    "    tok_texts = list([[w[0] for w in nlp_text]])\n",
    "    pos_texts = list([[w[1] for w in nlp_text]])\n",
    "\n",
    "    tok_texts = data_to_sequences(tok_texts, MAX_SEQUENCE_LENGTH, word_tokenizer)\n",
    "    pos_texts = data_to_sequences(pos_texts, MAX_SEQUENCE_LENGTH, pos_tokenizer)\n",
    "\n",
    "    # Make the prediction\n",
    "    pred_entities = model.predict({'word_input': tok_texts, 'pos_input': pos_texts})\n",
    "    pred_entities = [np.argmax(s, axis=1) for s in pred_entities]\n",
    "    \n",
    "    return pred_entities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_pred_to_str(nlp_text, bin_pred):\n",
    "    i = 0\n",
    "    \n",
    "    text_length = len(nlp_text)\n",
    "    entities = []\n",
    "\n",
    "    # Get list of entities separately\n",
    "    entities_stack = []\n",
    "    while i < text_length:\n",
    "        if bin_pred[i]:\n",
    "            entities_stack.append((i, nlp_text[i][2].text_with_ws))\n",
    "        i = i + 1\n",
    "\n",
    "    # Concatenate consecutive entities\n",
    "    last_pos = None\n",
    "    for e in entities_stack:\n",
    "        if last_pos == e[0] - 1:\n",
    "            last_pos = e[0]\n",
    "            n_entities = len(entities) - 1\n",
    "            entities[n_entities] = entities[n_entities] + e[1]\n",
    "        else:\n",
    "            last_pos = e[0]\n",
    "            entities.append(e[1])\n",
    "\n",
    "    entities = [e.strip() for e in entities]        \n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(text, model):\n",
    "    # Put the previous text inside of a list\n",
    "    texts = list([text])\n",
    "\n",
    "    # Tokenize it and get PoS tags\n",
    "    nlp_text = tokenize_texts(texts)[0]\n",
    "\n",
    "    pred_bin_text = make_bin_pred(model, nlp_text)\n",
    "    pred_str_entities = bin_pred_to_str(nlp_text, pred_bin_text)\n",
    "    \n",
    "    return pred_str_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['breast and ovarian cancer']\n"
     ]
    }
   ],
   "source": [
    "text = 'Germ-line mutations of the BRCA1 gene predispose women to early-onset breast '\\\n",
    "       'and ovarian cancer by compromising the genes presumptive function as a tumor suppressor'\n",
    "\n",
    "pred_str_entities = get_entities(text, model)\n",
    "\n",
    "print(pred_str_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "text = 'The patient has been diagnosed with scoliosis.'\n",
    "\n",
    "pred_str_entities = get_entities(text, model)\n",
    "\n",
    "print(pred_str_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_dir(model_name):\n",
    "    model_path = ARTIFACTS_PATH + model_name + '/'\n",
    "    if not os.path.exists(model_path):\n",
    "        os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_name, model):\n",
    "    create_model_dir(model_name)\n",
    "    model_path = ARTIFACTS_PATH + model_name + '/'\n",
    "    model.save_weights(model_path + 'model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokenizer(model_name, word_tokenizer, pos_tokenizer):\n",
    "    create_model_dir(model_name)\n",
    "    model_path = ARTIFACTS_PATH + model_name + '/'\n",
    "    \n",
    "    with open(model_path + 'word_tokenizer.pickle', 'wb') as fp:\n",
    "        pickle.dump(word_tokenizer, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    with open(model_path + 'pos_tokenizer.pickle', 'wb') as fp:\n",
    "        pickle.dump(pos_tokenizer, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "save_model(MODEL_NAME, model)\n",
    "save_tokenizer(MODEL_NAME, word_tokenizer, pos_tokenizer)\n",
    "print('Saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
