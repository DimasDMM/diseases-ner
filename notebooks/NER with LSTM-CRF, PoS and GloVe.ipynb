{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER with LSTM-CRF, PoS and GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Add functions in lib folder\n",
    "import sys\n",
    "import os\n",
    "module_path = os.path.abspath(os.path.join('../code'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from lib.DataProcess import DataProcess\n",
    "from lib.Jaccard import Jaccard\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed, Bidirectional, concatenate, Flatten\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.metrics import crf_accuracy\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train texts: 593\n",
      "Test texts: 100\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = '../data/'\n",
    "\n",
    "train_file = 'NCBI_corpus_training.txt'\n",
    "test_file = 'NCBI_corpus_testing.txt'\n",
    "\n",
    "with open(DATA_PATH + train_file, 'r') as fp:\n",
    "    train_dataset = fp.readlines()\n",
    "\n",
    "with open(DATA_PATH + test_file, 'r') as fp:\n",
    "    test_dataset = fp.readlines()\n",
    "\n",
    "print('Train texts: %d' % len(train_dataset))\n",
    "print('Test texts: %d' % len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard = Jaccard()\n",
    "data_process = DataProcess(jaccard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "This is a minimal preprocess of the data, as I do not want to remove any essential information from texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that I am using contains a number in the beginning of every text, thus I need to remove that number. Also, I replace the `category` tags with `<entity>`, so I can add them to the vocabular of Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identification of APC2, a homologue of the <entity>adenomatous polyposis coli tumour</entity> suppressor .\tThe <entity>adenomatous polyposis coli ( APC ) tumour</entity>-suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta ( GSK-3beta ) , axin / conductin and betacatenin . Complex formation induces the rapid degradation of betacatenin . In <entity>colon carcinoma</entity> cells , loss of APC leads to the accumulation of betacatenin in the nucleus , where it binds to and activates the Tcf-4 transcription factor ( reviewed in [ 1 ] [ 2 ] ) . Here , we report the identification and genomic structure of APC homologues . Mammalian APC2 , which closely resembles APC in overall domain structure , was functionally analyzed and shown to contain two SAMP domains , both of which are required for binding to conductin . Like APC , APC2 regulates the formation of active betacatenin-Tcf complexes , as demonstrated using transient transcriptional activation assays in APC - / - <entity>colon carcinoma</entity> cells . Human APC2 maps to chromosome 19p13 . 3 . APC and APC2 may therefore have comparable functions in development and <entity>cancer</entity> .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = data_process.apply_initial_cleaner(train_dataset)\n",
    "test_dataset = data_process.apply_initial_cleaner(test_dataset)\n",
    "\n",
    "print(train_dataset[0]) # Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I split the texts using the **Spacy Tokenizer**. Note that I split not only the words but also the sentences, so I consider each sentence as an independent input of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('identification', 'NOUN', Identification), ('of', 'ADP', of), ('apc2', 'PROPN', APC2), (',', 'PUNCT', ,), ('a', 'DET', a), ('homologue', 'NOUN', homologue), ('of', 'ADP', of), ('the', 'DET', the), ('<entity>', 'X', <entity>), ('adenomatous', 'ADJ', adenomatous), ('polyposis', 'NOUN', polyposis), ('coli', 'NOUN', coli), ('tumour', 'NOUN', tumour), ('</entity>', 'X', </entity>), ('suppressor', 'NOUN', suppressor), ('.', 'PUNCT', .)]\n"
     ]
    }
   ],
   "source": [
    "tok_train_dataset = data_process.tokenize_texts(train_dataset)\n",
    "tok_test_dataset = data_process.tokenize_texts(test_dataset)\n",
    "\n",
    "print(tok_train_dataset[0]) # Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's prepare the input of the model:\n",
    "- List of words.\n",
    "- List of 1s and 0s, where 1 indicate that there is an entity and 0 that it is just a normal word.\n",
    "- PoS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['identification', 'of', 'apc2', ',', 'a', 'homologue', 'of', 'the', 'adenomatous', 'polyposis', 'coli', 'tumour', 'suppressor', '.']\n"
     ]
    }
   ],
   "source": [
    "train_words = data_process.get_texts_words(tok_train_dataset)\n",
    "test_words = data_process.get_texts_words(tok_test_dataset)\n",
    "\n",
    "train_pos = data_process.get_texts_pos(tok_train_dataset)\n",
    "test_pos = data_process.get_texts_pos(tok_test_dataset)\n",
    "\n",
    "print(train_words[0]) # Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 1 1 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "train_indicators = data_process.get_indicator_sequences(tok_train_dataset)\n",
    "test_indicators = data_process.get_indicator_sequences(tok_test_dataset)\n",
    "\n",
    "print(train_indicators[0]) # Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model only accepts numbers, thus I must to transform each word into a unique number. Similarly, I repeat the process with the PoS sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional = [DataProcess.UNK]\n",
    "\n",
    "vocab_words, word2id, vocab_words_size = data_process.get_vocab_dictionary(train_words, additional)\n",
    "vocab_pos, pos2id, vocab_pos_size = data_process.get_vocab_dictionary(train_pos, additional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4093, 5418, 1287, 14, 904, 3963, 5418, 7171, 1026, 5844, 2104, 7367, 7047, 100]\n"
     ]
    }
   ],
   "source": [
    "train_words_enc = data_process.encode_texts(train_words, word2id)\n",
    "test_words_enc = data_process.encode_texts(test_words, word2id)\n",
    "\n",
    "train_pos_enc = data_process.encode_texts(train_pos, pos2id)\n",
    "test_pos_enc = data_process.encode_texts(test_pos, pos2id)\n",
    "\n",
    "print(train_words_enc[0]) # Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I use the technique **zero-padding** such that all sequences are in the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words_enc = data_process.to_sequences(train_words_enc, DataProcess.MAX_SEQUENCE_LENGTH)\n",
    "test_words_enc = data_process.to_sequences(test_words_enc, DataProcess.MAX_SEQUENCE_LENGTH)\n",
    "train_pos_enc = data_process.to_sequences(train_pos_enc, DataProcess.MAX_SEQUENCE_LENGTH)\n",
    "test_pos_enc = data_process.to_sequences(test_pos_enc, DataProcess.MAX_SEQUENCE_LENGTH)\n",
    "train_indicators = data_process.to_sequences(train_indicators, DataProcess.MAX_SEQUENCE_LENGTH)\n",
    "test_indicators = data_process.to_sequences(test_indicators, DataProcess.MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training the network we also need to change the bin_<>_dataset to categorial.\n",
    "train_indicators_cat = data_process.to_categorical(train_indicators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'embedding_dim': 300,\n",
    "    'lstm_cells': 128,\n",
    "    'word_lstm_dropout': 0.3,\n",
    "    'word_lstm_rec_dropout': 0.3,\n",
    "    'pos_lstm_dropout': 0.3,\n",
    "    'pos_lstm_rec_dropout': 0.3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_matrix(vocab_words, embedding_dim=300):\n",
    "    embeddings_index = {}\n",
    "\n",
    "    print('Reading GloVe file...')\n",
    "    \n",
    "    with open(DATA_PATH + 'glove.840B.{}d.txt'.format(embedding_dim)) as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            i = i + 1\n",
    "            if i % 100000 == 0:\n",
    "                print('- At line %d' % i)\n",
    "            \n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "\n",
    "            coefs = np.array(values[1:])\n",
    "            coefs[coefs == '.'] = '0.0'\n",
    "            coefs = np.asarray(coefs, dtype='float32')\n",
    "\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    print('Building embedding matrix...')\n",
    "    embedding_matrix = np.zeros((len(vocab_words) + 1, embedding_dim))\n",
    "    found_embeddings = 0\n",
    "    for i, word in enumerate(vocab_words):\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            found_embeddings = found_embeddings + 1\n",
    "\n",
    "    print('- Found %d embeddings of %d words' % (found_embeddings, len(vocab_words)))\n",
    "\n",
    "    return embedding_matrix, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading GloVe file...\n",
      "- At line 100000\n",
      "- At line 200000\n",
      "- At line 300000\n",
      "- At line 400000\n",
      "- At line 500000\n",
      "- At line 600000\n",
      "- At line 700000\n",
      "- At line 800000\n",
      "- At line 900000\n",
      "- At line 1000000\n",
      "- At line 1100000\n",
      "- At line 1200000\n",
      "- At line 1300000\n",
      "- At line 1400000\n",
      "- At line 1500000\n",
      "- At line 1600000\n",
      "- At line 1700000\n",
      "- At line 1800000\n",
      "- At line 1900000\n",
      "- At line 2000000\n",
      "- At line 2100000\n",
      "Building embedding matrix...\n",
      "- Found 6103 embeddings of 7791 words\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, _ = load_embedding_matrix(vocab_words, model_params['embedding_dim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model_params: dictionary:\n",
    "- embedding_dim\n",
    "- lstm_cells\n",
    "- word_lstm_dropout\n",
    "- word_lstm_rec_dropout\n",
    "- pos_lstm_dropout\n",
    "- pos_lstm_rec_dropout\n",
    "\"\"\"\n",
    "def create_model(model_params, vocab_words_size, vocab_pos_size, sequence_length, embedding_matrix):\n",
    "    word_input = Input(shape=(sequence_length,), name='words_input')\n",
    "    word_pipe = Embedding(input_dim=vocab_words_size + 1,\n",
    "                          output_dim=model_params['embedding_dim'],\n",
    "                          weights=[embedding_matrix],\n",
    "                          trainable=True)(word_input)\n",
    "    word_pipe = Bidirectional(\n",
    "                    LSTM(model_params['lstm_cells'],\n",
    "                         return_sequences=True,\n",
    "                         dropout=model_params['word_lstm_dropout'],\n",
    "                         recurrent_dropout=model_params['word_lstm_rec_dropout']),\n",
    "                    merge_mode='concat')(word_pipe)\n",
    "    word_pipe = TimeDistributed(Flatten())(word_pipe)\n",
    "\n",
    "    pos_input = Input(shape=(sequence_length,), name='pos_input')\n",
    "    pos_pipe = Embedding(input_dim=vocab_pos_size + 1,\n",
    "                         output_dim=model_params['embedding_dim'],\n",
    "                         input_length=sequence_length,\n",
    "                         trainable=True)(pos_input)\n",
    "    pos_pipe = Bidirectional(\n",
    "                    LSTM(model_params['lstm_cells'],\n",
    "                         return_sequences=True,\n",
    "                         dropout=model_params['pos_lstm_dropout'],\n",
    "                         recurrent_dropout=model_params['pos_lstm_rec_dropout']),\n",
    "                    merge_mode='concat')(pos_pipe)\n",
    "    pos_pipe = TimeDistributed(Flatten())(pos_pipe)\n",
    "    \n",
    "    # Concatenate both inputs\n",
    "    comb_pipe = concatenate([word_pipe, pos_pipe])\n",
    "\n",
    "    # Main BiLSTM model\n",
    "    comb_pipe = Bidirectional(\n",
    "        LSTM(model_params['lstm_cells'], return_sequences=True),\n",
    "        merge_mode='concat')(comb_pipe)\n",
    "    comb_pipe = TimeDistributed(Dense(64))(comb_pipe)\n",
    "    \n",
    "    output = CRF(2, name='output')(comb_pipe)\n",
    "    \n",
    "    model = Model(inputs=[word_input, pos_input], outputs=output)\n",
    "    model.compile(\n",
    "        loss=crf_loss,\n",
    "        optimizer='adam',\n",
    "        metrics=[crf_accuracy]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "I did not optimize the following parameters. If you want to get the best possible results, use some library to find the optimal parameter values: https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview#whats_a_hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words_input (InputLayer)        (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pos_input (InputLayer)          (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 300)     2337600     words_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 100, 300)     5700        pos_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 100, 256)     439296      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 100, 256)     439296      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 100, 256)     0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 100, 256)     0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 100, 512)     0           time_distributed_1[0][0]         \n",
      "                                                                 time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 100, 256)     656384      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 100, 64)      16448       bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "output (CRF)                    (None, 100, 2)       138         time_distributed_3[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 3,894,862\n",
      "Trainable params: 3,894,862\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = create_model(model_params,\n",
    "                     vocab_words_size,\n",
    "                     vocab_pos_size,\n",
    "                     DataProcess.MAX_SEQUENCE_LENGTH,\n",
    "                     embedding_matrix)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dimasdmm/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:430: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6736/6736 [==============================] - 268s 40ms/step - loss: 0.0544 - crf_accuracy: 0.9822\n",
      "Epoch 2/5\n",
      "6736/6736 [==============================] - 266s 39ms/step - loss: 0.0121 - crf_accuracy: 0.9964\n",
      "Epoch 3/5\n",
      "6736/6736 [==============================] - 264s 39ms/step - loss: 0.0062 - crf_accuracy: 0.9981\n",
      "Epoch 4/5\n",
      "6736/6736 [==============================] - 268s 40ms/step - loss: 0.0036 - crf_accuracy: 0.9987\n",
      "Epoch 5/5\n",
      "6736/6736 [==============================] - 266s 40ms/step - loss: 0.0019 - crf_accuracy: 0.9991\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "# Add early stop\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)\n",
    "\n",
    "history = model.fit(\n",
    "    {'words_input': train_words_enc, 'pos_input': train_pos_enc},\n",
    "    train_indicators_cat,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "I evaluate the model by computing the Jaccard Index with the pseudo-binary sequences. Details: https://en.wikipedia.org/wiki/Jaccard_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_indicators_cat = model.predict({'words_input': test_words_enc, 'pos_input': test_pos_enc})\n",
    "test_pred_indicators = np.array([np.argmax(s, axis=-1) for s in test_pred_indicators_cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard score: 0.8435\n"
     ]
    }
   ],
   "source": [
    "jaccard_score = jaccard.bin_jaccard(test_indicators, test_pred_indicators)\n",
    "print('Jaccard score: %.4f' % jaccard_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'lstm-crf-glove'\n",
    "\n",
    "model_path = '../artifacts/' + MODEL_NAME + '/'\n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)\n",
    "\n",
    "# Save model\n",
    "model.save(model_path + 'model.h5')\n",
    "\n",
    "# Save vocabularies\n",
    "with open(model_path + 'word_vocab.pickle', 'wb') as fp:\n",
    "    pickle.dump(vocab_words, fp)\n",
    "with open(model_path + 'pos_vocab.pickle', 'wb') as fp:\n",
    "    pickle.dump(vocab_pos, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
