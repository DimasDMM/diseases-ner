{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data overview\n",
    "\n",
    "This is just a **quick overview** of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import string\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as STOP_WORDS_SET\n",
    "from spacy.symbols import ORTH, LEMMA, POS\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed, Reshape, Bidirectional, concatenate, Flatten\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.metrics import crf_accuracy\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train texts: 593\n",
      "Test texts: 100\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = '../data/'\n",
    "\n",
    "train_file = 'NCBI_corpus_training.txt'\n",
    "test_file = 'NCBI_corpus_testing.txt'\n",
    "\n",
    "with open(DATA_PATH + train_file, 'r') as fp:\n",
    "    train_dataset = fp.readlines()\n",
    "\n",
    "with open(DATA_PATH + test_file, 'r') as fp:\n",
    "    test_dataset = fp.readlines()\n",
    "\n",
    "print('Train texts: %d' % len(train_dataset))\n",
    "print('Test texts: %d' % len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY_START = '<entity>'\n",
    "ENTITY_END = '</entity>'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MODEL_NAME = 'ner-lstm-crf'\n",
    "\n",
    "UNK = '<UNK>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "This is a minimal preprocess of the data, as I do not want to remove any essential information from texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that I am using contains a number in the beginning of every text, thus I need to remove that number. Also, I replace the `category` tags with `<entity>`, so I can add them to the vocabular of Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_initial_cleaner(texts):\n",
    "    texts = [re.sub(r'^[0-9]+\\s*', '', text) for text in texts]\n",
    "    texts = [re.sub(r'<category=\".+?\">(.+?)</category>', r'{}\\1{}'.format(ENTITY_START, ENTITY_END), text) for text in texts]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identification of APC2, a homologue of the <entity>adenomatous polyposis coli tumour</entity> suppressor .\tThe <entity>adenomatous polyposis coli ( APC ) tumour</entity>-suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta ( GSK-3beta ) , axin / conductin and betacatenin . Complex formation induces the rapid degradation of betacatenin . In <entity>colon carcinoma</entity> cells , loss of APC leads to the accumulation of betacatenin in the nucleus , where it binds to and activates the Tcf-4 transcription factor ( reviewed in [ 1 ] [ 2 ] ) . Here , we report the identification and genomic structure of APC homologues . Mammalian APC2 , which closely resembles APC in overall domain structure , was functionally analyzed and shown to contain two SAMP domains , both of which are required for binding to conductin . Like APC , APC2 regulates the formation of active betacatenin-Tcf complexes , as demonstrated using transient transcriptional activation assays in APC - / - <entity>colon carcinoma</entity> cells . Human APC2 maps to chromosome 19p13 . 3 . APC and APC2 may therefore have comparable functions in development and <entity>cancer</entity> .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = apply_initial_cleaner(train_dataset)\n",
    "test_dataset = apply_initial_cleaner(test_dataset)\n",
    "\n",
    "print(train_dataset[0]) # Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I split the texts using the **Spacy Tokenizer**. Note that I split not only the words but also the sentences, so I consider each sentence as an independent input of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nlp_engine():\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    nlp.tokenizer.add_special_case(ENTITY_START, [{ORTH: ENTITY_START, LEMMA: u'<entity>', POS: u'X'}])\n",
    "    nlp.tokenizer.add_special_case(ENTITY_END, [{ORTH: ENTITY_END, LEMMA: u'</entity>', POS: u'X'}])\n",
    "    \n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(nlp_tokens):\n",
    "    pos = [w.pos_ for w in nlp_tokens]\n",
    "    words = [w.lemma_.strip() if w.lemma_ != \"-PRON-\" else w.lower_.strip() for w in nlp_tokens]\n",
    "    tokens = [(words[i].lower(), pos[i], nlp_tokens[i]) for i in range(len(nlp_tokens))]\n",
    "        \n",
    "    # Remove empty tokens\n",
    "    tokens = [token for token in tokens if token[0].strip() != '']\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(dataset, split_sentences=True):\n",
    "    tok_dataset = []\n",
    "    \n",
    "    nlp = get_nlp_engine()\n",
    "    \n",
    "    for index, text in enumerate(dataset):\n",
    "        text = re.sub(rf'({ENTITY_START}|{ENTITY_END})', r' \\1 ', text, re.UNICODE)\n",
    "        text = re.sub(r\"\\s+\", ' ', text.strip())  # Normalize white spaces\n",
    "        nlp_tokens = nlp(text)\n",
    "        \n",
    "        if split_sentences:\n",
    "            sentences = []\n",
    "            \n",
    "            entity_indicator = 0\n",
    "            sentence_stack = []\n",
    "            \n",
    "            for s in nlp_tokens.sents:\n",
    "                for token in s:\n",
    "                    if token.text == ENTITY_START:\n",
    "                        entity_indicator = entity_indicator + 1\n",
    "                    elif token.text == ENTITY_END:\n",
    "                        entity_indicator = entity_indicator - 1\n",
    "                \n",
    "                sentence_stack = sentence_stack + list(s)\n",
    "                if entity_indicator == 0:\n",
    "                    sentences.append(sentence_stack)\n",
    "                    sentence_stack = []\n",
    "            \n",
    "            for s in sentences:\n",
    "                tokens = get_tokens(s)\n",
    "                tok_dataset.append(tokens)\n",
    "        else:\n",
    "            tokens = get_tokens(nlp_tokens)\n",
    "            tok_dataset.append(tokens)\n",
    "    \n",
    "    return np.array(tok_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('identification', 'NOUN', Identification), ('of', 'ADP', of), ('apc2', 'PROPN', APC2), (',', 'PUNCT', ,), ('a', 'DET', a), ('homologue', 'NOUN', homologue), ('of', 'ADP', of), ('the', 'DET', the), ('<entity>', 'X', <entity>), ('adenomatous', 'ADJ', adenomatous), ('polyposis', 'NOUN', polyposis), ('coli', 'NOUN', coli), ('tumour', 'NOUN', tumour), ('</entity>', 'X', </entity>), ('suppressor', 'NOUN', suppressor), ('.', 'PUNCT', .)]\n"
     ]
    }
   ],
   "source": [
    "tok_train_dataset = tokenize_texts(train_dataset)\n",
    "tok_test_dataset = tokenize_texts(test_dataset)\n",
    "\n",
    "print(tok_train_dataset[0]) # Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts_words(tok_dataset, skip_entity_tags=True):\n",
    "    if skip_entity_tags:\n",
    "        skip_tokens = [ENTITY_START, ENTITY_END]\n",
    "    else:\n",
    "        skip_tokens = []\n",
    "    \n",
    "    texts_words = [[token[0] for token in text if token[0] not in skip_tokens] for text in tok_dataset]\n",
    "    return np.array(texts_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['identification', 'of', 'apc2', ',', 'a', 'homologue', 'of', 'the', 'adenomatous', 'polyposis', 'coli', 'tumour', 'suppressor', '.']\n"
     ]
    }
   ],
   "source": [
    "train_words = get_texts_words(tok_train_dataset)\n",
    "test_words = get_texts_words(tok_test_dataset)\n",
    "\n",
    "print(train_words[0]) # Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_dictionary(data_words, additional_words=[], num_words=150000, min_count=1):\n",
    "    counter_words = Counter()\n",
    "    for words in data_words:\n",
    "        counter_words.update(words)\n",
    "\n",
    "    vocab_words_count = {w for w, c in counter_words.items() if c >= min_count}\n",
    "    vocab_words = sorted(list(vocab_words_count))\n",
    "    \n",
    "    vocab_words = [''] + list(additional_words) + vocab_words\n",
    "    if len(vocab_words) - 1 > num_words:\n",
    "        vocab_words = vocab_words[1:(num_words + 1)]\n",
    "    \n",
    "    word2id = {word.strip(): idx for idx, word in enumerate(vocab_words)}\n",
    "    \n",
    "    del word2id['']\n",
    "    del vocab_words[0]\n",
    "    \n",
    "    vocab_size = len(vocab_words)\n",
    "    \n",
    "    return vocab_words, word2id, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional = [UNK]\n",
    "\n",
    "vocab_words, word2id, vocab_words_size = get_vocab_dictionary(train_words, additional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 7791\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size: %d' % vocab_words_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **quick plot** of the data shows up that almost all sentences are less than $50$ tokens in length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_words = list(train_words) + list(test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.hist([len(s) for s in full_words], bins=50)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
