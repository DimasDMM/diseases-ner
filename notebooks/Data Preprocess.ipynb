{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocess\n",
    "\n",
    "Take into account that all this analysis and preprocess depends on the language that you are working with. In this case, the original dataset is in **English**.\n",
    "\n",
    "If you receive any error when trying to import `en_core_web_sm`, try executing this command first: `python -m spacy download en_core_web_sm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import string\n",
    "import en_core_web_sm\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as STOP_WORDS_SET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Read the dataset\n",
    "\n",
    "Despite you will find your dataset in a CSV, in this respository the data is stored in a database. Here I have defined a few functions to manage the connection with the database and parse the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train texts: 593\n",
      "Test texts: 100\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = '../data/'\n",
    "\n",
    "train_file = 'NCBI_corpus_training.txt'\n",
    "test_file = 'NCBI_corpus_testing.txt'\n",
    "\n",
    "with open(DATA_PATH + train_file, 'r') as fp:\n",
    "    train_dataset = fp.readlines()\n",
    "\n",
    "with open(DATA_PATH + test_file, 'r') as fp:\n",
    "    test_dataset = fp.readlines()\n",
    "\n",
    "print('Train texts: %d' % len(train_dataset))\n",
    "print('Test texts: %d' % len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "\n",
    "This is a minimal preprocess, as I do not want to remove any essential information from texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nlp_engine(basic_tokenizer=False):\n",
    "    if basic_tokenizer:\n",
    "        nlp = English()\n",
    "    else:\n",
    "        nlp = en_core_web_sm.load()\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that I am using contains a number in the beginning of every text. Here I remove that number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_initial_number_remover(texts):\n",
    "    texts = [re.sub(r'^[0-9]+\\s*', '', text) for text in texts]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = apply_initial_number_remover(train_dataset)\n",
    "test_dataset = apply_initial_number_remover(test_dataset)\n",
    "\n",
    "#print(train_dataset[0]) # Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentize texts in sentences, i.e. I am going to consider each sentence as an independent sample in the input of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sentences_split(texts):\n",
    "    nlp = get_nlp_engine()\n",
    "    nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "    \n",
    "    sentences = []\n",
    "    for text in texts:\n",
    "        t_sentences = nlp(text)\n",
    "        t_sentences = [s.text.strip() for s in t_sentences.sents if s.text.strip() != '']\n",
    "        sentences = sentences + t_sentences\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = apply_sentences_split(train_dataset)\n",
    "test_dataset = apply_sentences_split(test_dataset)\n",
    "\n",
    "#print(train_dataset[0:5]) # Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I clean each sentence, I get all tags \"*category*\" from them, so I can use them **to measure the performance** of my NER algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns a list \"text ID -> list of entities\" where list of entities\n",
    "is a list of tuples like \"(category name, text in tag)\"\n",
    "\"\"\"\n",
    "def get_entities_in_category_tags(texts):\n",
    "    entities = []\n",
    "    for text_id, text in enumerate(texts):\n",
    "        t_entities = re.findall(r'<category=\"(.+?)\">(.+?)</category>', text)\n",
    "        entities.append(t_entities)\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_entities = get_entities_in_category_tags(train_dataset)\n",
    "test_entities = get_entities_in_category_tags(test_dataset)\n",
    "\n",
    "#print(train_entities[0]) # Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's **remove the \"*category*\" tags** as well as **stop words** and **punctuation symbols**. Also, I **normalize words** which contain some capital letters: the first letter will be capitalized and the rest will be in lower cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tag_cleaner(texts):\n",
    "    texts = texts.copy()\n",
    "    for index, text in enumerate(texts):\n",
    "        text = re.sub(r'<category=\".+?\">', ' ', text)\n",
    "        text = re.sub(r'</category>', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        texts[index] = text.strip()\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = apply_tag_cleaner(train_dataset)\n",
    "test_dataset = apply_tag_cleaner(test_dataset)\n",
    "\n",
    "#print(train_dataset[0]) # Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the corpus in two different ways:\n",
    "- As 1s and 0s, where $1$ indicate that there is an entity and $0$ that it is just a normal word.\n",
    "- As PoS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(texts, basic_tokenizer=False):\n",
    "    texts = texts.copy()\n",
    "    nlp = get_nlp_engine(basic_tokenizer=basic_tokenizer)\n",
    "    \n",
    "    for index, text in enumerate(texts):\n",
    "        nlp_tokens = nlp(text)\n",
    "        \n",
    "        pos = [w.pos_ for w in nlp_tokens]\n",
    "        lemmas = [w.lemma_.strip() if w.lemma_ != \"-PRON-\" else w.lower_.strip() for w in nlp_tokens]\n",
    "        lemmas = [w if any(c.isupper() for c in w) else w for w in lemmas]\n",
    "        \n",
    "        # Remove empty tokens\n",
    "        tokens = [(lemmas[i].strip(), pos[i], nlp_tokens[i]) for i in range(len(nlp_tokens)) if lemmas[i].strip() != '']\n",
    "        \n",
    "        texts[index] = tokens\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bin_exact_dataset(dataset, entities):\n",
    "    tokenized_dataset = tokenize_texts(dataset, basic_tokenizer=True)\n",
    "    tokenized_dataset = [[w[0] for w in text] for text in tokenized_dataset]\n",
    "    \n",
    "    bin_entities_per_text = []\n",
    "\n",
    "    for text_i, tokens in enumerate(tokenized_dataset):\n",
    "        len_tokens = len(tokens)\n",
    "\n",
    "        bin_entities = np.zeros(len_tokens, dtype=int)\n",
    "\n",
    "        token_i = 0\n",
    "        for tup_entity in entities[text_i]:\n",
    "            token_start = -1\n",
    "            token_stop = -1\n",
    "            \n",
    "            entity = tokenize_texts([tup_entity[1]], basic_tokenizer=True)[0]\n",
    "            entity = [w[0] for w in entity]\n",
    "            \n",
    "            ent_i = 0\n",
    "            while token_i < len_tokens:\n",
    "                ### Fixed values: there are some mismatches :/ ###\n",
    "                # TO DO: Fix tokenizer so I do not need to tokenize the entities separately\n",
    "                if tokens[token_i] == 'Iowa':\n",
    "                    tokens[token_i] = 'Ia'\n",
    "                ###\n",
    "                \n",
    "                if entity[ent_i].lower() == tokens[token_i].lower():\n",
    "                    if token_start == -1:\n",
    "                        token_start = token_i\n",
    "                    \n",
    "                    ent_i = ent_i + 1\n",
    "                    token_i = token_i + 1\n",
    "                    \n",
    "                    if ent_i == len(entity):\n",
    "                        token_stop = token_i\n",
    "                        break\n",
    "                else:\n",
    "                    token_start = -1\n",
    "                    token_i = token_i + 1\n",
    "            \n",
    "            if token_stop == -1:\n",
    "                print(tokens)\n",
    "                raise Exception('Entity not found:', entity)\n",
    "            else:\n",
    "                bin_entities[range(token_start, token_stop)] = 1\n",
    "\n",
    "        bin_entities_per_text.append(bin_entities)\n",
    "    \n",
    "    return bin_entities_per_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_train_dataset = get_bin_exact_dataset(train_dataset, train_entities)\n",
    "bin_test_dataset = get_bin_exact_dataset(test_dataset, test_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data overview\n",
    "\n",
    "A quick overview of the data shows up that almost all sentences are less than $50$ tokens in length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_bin_dataset = bin_train_dataset + bin_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.hist([len(s) for s in full_bin_dataset], bins=50)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save preprocessed dataset\n",
    "\n",
    "Finally, I **save the preprocess results**, so I can use it in the next steps (to train a model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_LINE_CHAR = \"\\n\"\n",
    "STR_JOIN_CHAR = ' '\n",
    "\n",
    "corpus_train_file = 'corpus_train.txt'\n",
    "corpus_test_file = 'corpus_test.txt'\n",
    "bin_corpus_train_file = 'bin_corpus_train.txt'\n",
    "bin_corpus_test_file = 'bin_corpus_test.txt'\n",
    "\n",
    "PARSED_DATA_PATH = DATA_PATH + 'parsed/'\n",
    "if not os.path.exists(PARSED_DATA_PATH):\n",
    "    os.mkdir(PARSED_DATA_PATH)\n",
    "\n",
    "# Store full parsed texts\n",
    "with open(PARSED_DATA_PATH + corpus_train_file, 'w') as fp:\n",
    "    fp.write(NEW_LINE_CHAR.join(train_dataset))\n",
    "\n",
    "with open(PARSED_DATA_PATH + corpus_test_file, 'w') as fp:\n",
    "    fp.write(NEW_LINE_CHAR.join(test_dataset))\n",
    "\n",
    "# Store corpus as 1s and 0s\n",
    "with open(PARSED_DATA_PATH + bin_corpus_train_file, 'w') as fp:\n",
    "    bin_lines = [STR_JOIN_CHAR.join(t.astype('str')) for t in bin_train_dataset]\n",
    "    fp.write(NEW_LINE_CHAR.join(bin_lines))\n",
    "\n",
    "with open(PARSED_DATA_PATH + bin_corpus_test_file, 'w') as fp:\n",
    "    bin_lines = [STR_JOIN_CHAR.join(t.astype('str')) for t in bin_test_dataset]\n",
    "    fp.write(NEW_LINE_CHAR.join(bin_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
